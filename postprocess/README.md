# Postprocessing for NA-CORDEX-CMIP6
----------------------------------


## Current workflow and usage instructions 

### Part 1: Extact variables from WRF output files and write CF- and CORDEX-compliant files

1. Edit `write.cmd.py` to include the time range (in years) and 
   variables of interest for postprocessing. Make sure to add the correct
   `wrfout_path` which points to the location of the wrf_d01 data. This
   workflow works with the directory structure of NA-CORDEX-CMIP6 runs 
   and would need to be modified to work for other wrf runs. 

2. Run `write.cmd.py` to generate a command file for launch_cf.

3. Inspect your command file to be sure it's what you expect. 

4. Run launch_cf: to divert stderr/stdout, this is the recommended
   syntax: <br/>
   `launch_cf -A ${PBS_ACCOUNT} -l walltime=1:00:00 --mem
   150GB -o /dev/null -e /dev/null -k oe ./cmdfile`

5. The script takes some time for the year (15 minutes) and takes 
   even longer for precip because it's aggregating multiple hourly
   variables from WRF. 


### Part 2: Aggregate hourly data to daily and daily to monthly

6. Run `aggregate.sh $indir $outdir $cmddir` where:

   * `$indir` contains the directories (one per variable) generated by
     `write.cmd.py`
   * `$outdir` is the directory where you want to generate a tree of output files
   * `$cmddir` is the directory where you want the commandfiles

   This will generate a set of commandfiles to aggregate hourly
   variables to daily and place them in a directory tree following the
   CORDEX spec.  We recommend doing this in your scratch space, just
   in case something goes wrong.

7. Use `launch_multi` to run the commandfiles: <br>
   `launch_multi --run $rundir $cmddir/*cmd`

   This iterates through the command files and for each one, creates a
   directory (in `$rundir`) named after the commandfile, copies the
   commandfile and `~/config_env.sh` to the run directory, and runs
   `launch_cf` from there, so all your captured output ends up neatly
   contained.

   (You need a file named `config_env.sh` containing the command
   `module restore default` to get access to the modules -- like CDO
   -- that have all the software you need.  You can also activate
   conda environments, etc. in it.)

8. After all the jobs have finished, check the files in the run
   directory.  If all of the `jobname.o[number].N` files end with a line
   that starts `Done`, and all the `step-NNNNN.out` files in the stdout
   directory are empty, you're all set.  (You can check this quickly
   using `wc` and `grep | cut | uniq -c`)  Otherwise, debug and try again.


9. Run `aggregate.sh` a second time to generate daily -> monthly
   commandfiles

10. Use `launch_multi` as before on the new commandfiles.

11. Check the outputs again.

12. [make plots to check that the data looks good]

13. Move the files to their final location.  The easiest way to do
    this is using Globus via the web UI, which will do it
    autonomously, in parallel, and with error checking.  Detailed
    instructions can be found in [NCAR's HPC
    documentation](https://ncar-hpc-docs.readthedocs.io/en/latest/storage-systems/data-transfer/globus/#transferring-files-with-the-web-interface).

### List of current known problems: 
1.  Discrepancy in cell_methods from WRF output and WCRP guidelines

    As the variable attributes are read in from the official WCRP JSON
    tables, there are currently disconnects between the stated
    attribute of a variable and the actual attribute of that variable
    come from WRF.

    The post-processing script does correct the units of the core
    variables, but the likely discrepancy is in the "cell_methods"
    attribute. This is important, because of how variables with
    different cell methods are given time_bnds and the time dimension
    is directly edited to provide the needed information on
    time-scales for means and maximimums.

    I think, the current problematic variables are:

    a. clt (cloud fraction)
    b. evspsbl (evaporation including subplimation and transpiration) 
